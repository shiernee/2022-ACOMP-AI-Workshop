{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3j4yhXEJLXk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shiernee/2022-ACOMP-AI-Workshop/blob/main/2022_Workshop_Day2.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\\\\\n",
    "\n",
    "This notebook is modified from [Course material for the 2020 instance of the Data analysis with python course by the Univeristy of Helsinki](https://github.com/csmastersUH/data_analysis_with_python_2020/blob/master/linear_regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQIShu_iJLXn"
   },
   "source": [
    "# Linear regression\n",
    "Regression analysis tries to explain relationships between variables. One of these variables, called dependend variable, is what we want to \"explain\" using one or more *explanatory variables*. In linear regression we assume that the dependent variable can be, approximately, expressed as a linear combination of the explanatory variables. As a simple example, we might have dependent variable height and an explanatory variable age. The age of a person can quite well explain the height of a person, and this relationship is approximately linear for kids (ages between 1 and 16). Another way of thinking about regression is fitting a curve to the observed data points. If we have only one explanatory variable, then this is easy to visualize, as we shall see below.\n",
    "\n",
    "We can apply the linear regression easily with the [scikit-learn](https://scikit-learn.org/stable/) package. Let's go through some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDKStIcLJLXo"
   },
   "source": [
    "First we make the usual standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:31:02.144483Z",
     "start_time": "2020-06-24T19:31:01.933556Z"
    },
    "id": "-DiIAQDnJLXp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn   # This imports the scikit-learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cMaimZiJLXq"
   },
   "source": [
    "Then we create some data with approximately the relationship $y=2x+1$, with normally distributed errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:31:02.150290Z",
     "start_time": "2020-06-24T19:31:02.145821Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDmmu4rVJLXq",
    "outputId": "ab385876-d310-469b-f339-f4607fd440ae"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n=20   # Number of data points\n",
    "x=np.linspace(0, 10, n)\n",
    "y=x*2 + 1 + 1*np.random.randn(n) # Standard deviation 1\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uiFMU_4JLXr"
   },
   "source": [
    "Next we import the `LinearRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:31:02.253488Z",
     "start_time": "2020-06-24T19:31:02.152025Z"
    },
    "id": "eU3veZtvJLXs"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d_bvBg9JLXt"
   },
   "source": [
    "Now we can fit a line through the data points (x, y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:31:02.350861Z",
     "start_time": "2020-06-24T19:31:02.254992Z"
    },
    "id": "9-Ez3M_oJLXt",
    "outputId": "26608cba-df25-4c1b-b0a3-1a6daebc75bc"
   },
   "outputs": [],
   "source": [
    "model=LinearRegression(fit_intercept=True)\n",
    "model.fit(x[:,np.newaxis], y)\n",
    "xfit=np.linspace(0,10,100)\n",
    "yfit=model.predict(xfit[:, np.newaxis])\n",
    "plt.plot(xfit,yfit, color=\"black\")\n",
    "plt.plot(x,y, 'o')\n",
    "# The following will draw as many line segments as there are columns in matrices x and y\n",
    "plt.plot(np.vstack([x,x]), np.vstack([y, model.predict(x[:, np.newaxis])]), color=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO2ZfWztJLXu"
   },
   "source": [
    "The linear regression tries to minimize the sum of squared errors $\\sum_i (y[i] - \\hat{y}[i])^2$; this is the sum of the squared lengths of the red line segments in the above plot. The estimated values $\\hat{y}[i]$ are denoted by `yfit[i]` in the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:31:02.355766Z",
     "start_time": "2020-06-24T19:31:02.352189Z"
    },
    "id": "i-Grq7dWJLXu",
    "outputId": "c1403e6e-82d5-4ea2-a9ed-c79529171d33"
   },
   "outputs": [],
   "source": [
    "print(\"Parameters:\", model.coef_, model.intercept_)\n",
    "print(\"Coefficient:\", model.coef_[0])\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmK0lg6HJLXv"
   },
   "source": [
    "In this case, the coefficient is the slope of the fitted line, and the intercept is the point where the fitted line intersects with the y-axis.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Note that in scikit-learn the attributes of the model that store the learned parameters have always an underscore at the end of the name. This applies to all algorithms in sklearn, not only the linear regression. This naming style allows one to easily spot the learned model parameters from other attributes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLcRe-N4JLXv"
   },
   "source": [
    "The parameters estimated by the regression algorithm were quite close to the parameters that generated the data: coefficient 2 and intercept 1. Try experimenting with the number of data points and/or the standard deviation, to see if you can improve the estimated parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET-6gXwzJLXw"
   },
   "source": [
    "### Multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfVRNHnyJLXw"
   },
   "source": [
    "The previous example had only one explanatory variable. Sometimes this is called a *simple linear regression*. The next example illustrates a more complex regression with multiple explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:31:02.360803Z",
     "start_time": "2020-06-24T19:31:02.357229Z"
    },
    "id": "cthbys8MJLXw"
   },
   "outputs": [],
   "source": [
    "sample1=np.array([1,2,3])   # The three explanatory variables have values 1, 2, and 3, respectively\n",
    "sample2=np.array([4,5,6])   # Another example of values of explanatory variables\n",
    "sample3=np.array([7,8,10])   # ...\n",
    "y=np.array([15,39,66]) + np.random.randn(3)   # For values 1,2, and 3 of explanatory variables, the value y=15 was observed, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ5rYGpeJLXx"
   },
   "source": [
    "Let's try to fit a linear model to these points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:31:02.366671Z",
     "start_time": "2020-06-24T19:31:02.361957Z"
    },
    "id": "MC6hbE2iJLXx",
    "outputId": "ff7717c3-7489-4421-bc85-b6bbb88fff3d"
   },
   "outputs": [],
   "source": [
    "model2=LinearRegression(fit_intercept=False)\n",
    "x=np.vstack([sample1,sample2,sample3])\n",
    "model2.fit(x, y)\n",
    "model2.coef_, model2.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiD-8kKAJLXy"
   },
   "source": [
    "Let's print the various components involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:31:02.373120Z",
     "start_time": "2020-06-24T19:31:02.368636Z"
    },
    "id": "sDXnoZD3JLXy",
    "outputId": "a92a9204-655d-415d-8a0a-37f9b2c1bfc0"
   },
   "outputs": [],
   "source": [
    "b=model2.coef_[:, np.newaxis]\n",
    "print(\"x:\\n\", x)\n",
    "print(\"b:\\n\", b)\n",
    "print(\"y:\\n\", y[:, np.newaxis])\n",
    "print(\"product:\\n\", np.matmul(x, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99UOwaamJLXz"
   },
   "source": [
    "### Polynomial regression\n",
    "It may perhaps come as a surprise that one can fit a polynomial curve to data points\n",
    "using linear regression. The trick is to add new explanatory variables to the model.\n",
    "Below we have a single feature x with associated y values given by third degree polynomial,\n",
    "with some (gaussian) noise added. It is clear from the below plot that we cannot explain the data well with a linear function. We add two new features: $x^2$ and $x^3$. Now the model has three explanatory variables, $x, x^2$ and $x^3$. The linear regression will find the coefficients for these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T19:31:02.501116Z",
     "start_time": "2020-06-24T19:31:02.375078Z"
    },
    "id": "_vUMETH2JLXz",
    "outputId": "5c327f0b-3727-4ab4-d574-26b2deedc0fb"
   },
   "outputs": [],
   "source": [
    "x=np.linspace(-50,150,50)\n",
    "y=0.15*x**3 - 20*x**2 + 5*x - 4 + 5000*np.random.randn(50)\n",
    "plt.scatter(x, y, color=\"black\")\n",
    "model_linear=LinearRegression(fit_intercept=True)\n",
    "model_squared=LinearRegression(fit_intercept=True)\n",
    "model_cubic=LinearRegression(fit_intercept=True)\n",
    "x2=x**2\n",
    "x3=x**3\n",
    "model_linear.fit(np.vstack([x]).T, y)\n",
    "model_squared.fit(np.vstack([x,x2]).T, y)\n",
    "model_cubic.fit(np.vstack([x,x2,x3]).T, y)\n",
    "xf=np.linspace(-50,150, 50)\n",
    "yf_linear=model_linear.predict(np.vstack([x]).T)\n",
    "yf_squared=model_squared.predict(np.vstack([x,x2]).T)\n",
    "yf_cubic=model_cubic.predict(np.vstack([x,x2,x3]).T)\n",
    "plt.plot(xf,yf_linear, label=\"linear\")\n",
    "plt.plot(xf,yf_squared, label=\"squared\")\n",
    "plt.plot(xf,yf_cubic, label=\"cubic\")\n",
    "plt.legend()\n",
    "print(\"Coefficients:\", model_cubic.coef_)\n",
    "print(\"Intercept:\", model_cubic.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLwRA7UtJLX0"
   },
   "source": [
    "Linear and squared are not enough to explain the data, but the linear regression manages to fit quite well a polynomial curve to the data points, when cubic variables are included!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slhxqc9FJLX1"
   },
   "source": [
    "## Additional information\n",
    "\n",
    "* The [scikit-learn](https://scikit-learn.org/stable/) library concentrates on machine learning. Check out library [statsmodels](http://www.statsmodels.org/stable/index.html) for a more statistical viewpoint to regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly_NBh26JLX2"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shiernee/2022-ACOMP-AI-Workshop/blob/main/2022_Workshop_Day2.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
